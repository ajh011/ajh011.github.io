<!DOCTYPE html>
<html>
  <head>
    <title>Neural Networks</title>
    <link href="../../assets/css/nn.css" rel="stylesheet">
    <link href="../../assets/css/ml.css" rel="stylesheet">
    <link href="../../assets/css/jumper.css" rel="stylesheet">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>MathJax.Hub.Config({  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}},
                                  loader: {load: ['[tex]/ams']},
                                  tex: {packages: {'[+]': ['ams']}});</script>
    
    <meta charset="UTF-8">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Monsieur+La+Doulaise&display=swap" rel="stylesheet">

    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="../../assets/images/res/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../../assets/images/res/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../../assets/images/res/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../assets/images/res/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="../../assets/images/res/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="../../assets/images/res/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="../../assets/images/res/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../../assets/images/res/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="../../assets/images/res/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="../../assets/images/res/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="../../assets/images/res/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="../../assets/images/res/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="../../assets/images/res/favicon-128.png" sizes="128x128" />
    <meta name="application-name" content="&nbsp;"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="../../assets/images/res/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="../../assets/images/res/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="../../assets/images/res/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo" content="../../assets/images/res/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo" content="../../assets/images/res/mstile-310x310.png" />



    <meta charset="UTF-8">
    <meta name="description" content="A discussion of why neural networks are interesting. Includes
                                      relevant resources for learning about neural networks
                                      and programming them in python.">
    <meta name="keywords" content="Neural Networks, Neural Nets, python Neural Nets,
                                   pytorch">
    <meta name="author" content="Alexander J. Heilman">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow">
  </head>
  
  <body>


    <div id="home">
      <a href="https://www.alexheilman.com"><img id="homeimg" src="../../assets/images/iconwhite/favicon-196x196.png" width="50"></a>
    </div>
  

    <h1 id="title">
      Neural Networks
    </h1>  
    <p id="explanation">
      Neural Networks have become a mainstay of modern machine learning. They're 
      especially interesting because of their basic and understandable components,
      as well as their loose association with an idealized brain. They're easily implementable
      on pytorch and tensorflow, both python based machine learning modules.  
    </p>
    <img class="center" src="../../assets/images/neural/ffnwhite.svg" alt="Example neural network" width="87%" height="auto" id="examplenn">
    <section id="basics">
      <h1 id="basicstit"> Neural Network Components  </h1>
      <p>
        Basic networks are really composed of rather simple objects. Like any
        network, neural nets have nodes, or neurons, that tend to apply relatively
        easy to compute function on a set of input data. In most models, each neuron applies some
        weighted sum to the input vector and then some non-linear activation function, which is then passed on to the 
        next layer of neurons (or read as output on the final layer).
      </p>
      <h2 id="neuronstit">Neurons</h2>
      <p >
        <div id="nleft" class="">
        The neuron is the basic building block of a neural network. The mathematical functions employed at each
        neuron are relatively simple, but their trainable connections are what give the overall networks their 
        computational power (much like a transistor in a computer is simple, or the neurons in our brains connect
        to form a complex structure). The prototypical neuron simply applies some non-linear activation function to a 
        weighted sum of it's inputs with some bias added, and passes this output to the next layer of neurons.
        
        <img src="../../assets/images/neural/neuronwbsmatwhite.svg" alt="Example neuron with weights and biases in matrix form" 
           width="80%" height="auto" id="exneuron" class="center"> 
        </div>
        
        Above, the example neuron \(j\) takes the inner product of a weight vector (learned through training, discussed below) with a vector 
        formed of it's \(n\) inputs, adds some bias \(b_j\), and then applies some non-linear activation function \(\sigma\). This is shown as output along 
        the departing connection.
      </p>
      <h3 id="activationfunctionstit">
        Activation Functions
      </h3>
      <p id="afp1">
        Much like the simple operations of linear algebra, the activation functions employed by neurons are generally simple: both to aid
        in computational complexity concerns when applying the model; and to streamline the 
        process of backpropogation, described later (and which is involved in the training process). Perhaps the two most commonly seen activation
        functions are the logarithmic Sigmoid function and the more modern and widely
        used rectified linear unit (ReLU).
      </p>
    <!--  <a href="https://commons.wikimedia.org/wiki/File:Sigmoid-function-2.svg" id="sigmoidlink" target="_blank" class="">    sigmoid     </a>
     -->
      <div id="actex">
        <img src="../../assets/images/neural/sigmoid.svg" alt="Example Sigmoid function plot" width="45%" height="auto" class="margin5">
        <img id="relu" src="../../assets/images/neural/relu.svg" alt="Example rectified linear unit function plot" width="45%" height="auto" class="margin5">
      </div>
      <p>
        It's important that the activation function of each layer is non-linear, since this introduced non-linearity makes the otherwise repeated matrix multiplication non-redundant
        (since any product of matrices is itself a matrix). Thus, the use of such simple activation functions (such as the ReLU) is rational since we only require they introduce some 
        non-linearity and not that they perform arbitrarily complex calculations.
      </p>
    
      <p id="bbnp2">
      <!-- words wrap left--> <img src="../../assets/images/neural/ffnwhiteoutput.svg" alt="" >
      </p>


        <h2> Layers </h2>
        <p>
          As hinted at above, these neurons are then placed in parallel to one another to form layers, which are then connected in series 
          to other layers. This layered structure is what gives neural networks their power to compute arbitrary functions (after training with
          adequate quantities and quality of data).
        </p>
        <h3>Hidden Layers</h3>
        <p>
          Intermediate layers of networks (those not given as input or taken as output) are referred to as hidden layers. A given 
          neural network will consist of an arbitrary number of hidden layers (in series) of arbitrary width (number of stacked/parallel neurons). 
          The width may vary from hidden layer to hidden layer.
        </p>
        <img class="center" src="../../assets/images/neural/layersex.svg" alt="Example neural network with layers distinguished" width="65%" height="auto" id="examplennlayers" class="">
        <p>
          In each intermediate layer of the neural network, the aggregation of
          inputs can be seen as a matrix acting
          on some input vector, as a natural extension of the single neuron case.
          The activation function (assuming it's the same for all the neurons) 
          can then simply act on this transformed vector via element-wise distribution (though the parameters
          of the activation function may vary for each element of the vector).
        </p>
        <p>
          Below is a prototypical hidden layer of a neural network, with \(k\) parallel neurons and an \(n-\)dimensional input. Note that each neuron has it's own weight vector and bias associated 
          with it, which are then packaged as a weight matrix and a bias vector. Again, \(\sigma\) is the activation 
          function employed by this layer.
        </p>
        <img src="../../assets/images/neural/ffnwhiteinput.svg" alt="Basic computation done by a layer in a neural network" width="45%" height="auto"
          id="ffninput" class="center"> 
        <p>
          An arbitrary number of these layers then may be concatenated to form an entire network. Often times, the final layer, or output
          layer will employ a different function to normalize and ease interpretations of such outputs, though a distinction between different 
          potential tasks the network is employed in performing is appropriate here.
        </p>

        <h3> Output Layers</h3>
        <p>
          Since different networks are employed to perform different types of tasks, the actions performed on the output layers are often dependent on this distinction. The two most 
          common tasks are regression (numerical prediction) and classification (probabilistic prediction). Some common functions appplied in the output layer of networks designed for these tasks are discussed below.
        </p>
        <h4>Regression</h4>
        <p>
          A network tasked with regression is one that (in the simplest case) predicts a single numerical target. As such, the output layer is effectively just one node and a single, local activation funciton may 
          be employed. This may be the same function as that used in the hidden layers.
        </p>
        <img class="center" src="../../assets/images/neural/regout.svg" alt="Example regression output node" width="65%" height="auto" id="exampleregout" class="">
        <h4>Classification</h4>
        <p>
          A more interesting case is that of classification. A network designed for classification is tasked with determining which class a given input is in, and thus should, ideally, give a
          one-hot encoded vector as output. This rarely is attainable and thus some non-local operation may be appropriate. One may simply choose the highest valued output node, but more Often
          the network will incoporate a function that normalizes the output to allow for a probabilistic interpretation.
        </p>
        <img class="center" src="../../assets/images/neural/classout.svg" alt="Example classification output vector" width="65%" height="auto" id="exampleclassout" class="">

        <p>
          An example of this would be the SoftMax functon which takes the exponential of each 
          output and normalizes these relative their exponentiated sum. This is described mathematically as the following:
          \[
          \text{SoftMax}_{z}(z_j)=\frac{e^{z_j}}{\sum_i e^{z_i}}
          \]
          This provides an advatange in that it is better at distinguishing otherwise closely valued outputs better, but still normalizes the output and thus allows for 
          a probabilistic interpretation of the output vector. Below, the action of SoftMax is shown graphically as one output (before softmax, displayed along the x-axis) is varied.
          Note that the sum of all the outputs (plotted on the y-axis) remains one as it is varied.
        </p>
        <iframe src="https://www.desmos.com/calculator/wdipmg9s0q?embed" width="100%" height="400" invertedColors="true" style="border: 1px solid #ccc" id="softmaxinteractive" frameborder=0></iframe>



        <!--<iframe width="100%" height="400" src="https://www.desmos.com/calculator/qvltyv82p3" id="softmaxinteractive"></iframe>-->
  
    </section>

    <h2>Universal Approximation Theorems</h2>
    <p>
      A reasonable question one may have now is: 'is there any guarantee a given network is effective for some task?'. While a 'guarantee' is a strong condition,
      there are some promising theorems that suggest certain sets of networks are general enough to approximate arbitrary tasks.
    </p>
    <p>
      The class of universal approximation theorems are essentially trying to prove that certain tasks of interest can always be adequatley approximated
      by some class of neural network. 
    </p>
    <section id="training">
      <h1 id="traintit">Training</h1>
        <p id="trainp1"> 
          A neural network is analagous to a, potentially huge, set
          of knobs and dials. At this point, one may ask how we could ever 
          adjust such a large number of parameters to actually do something useful? And 
          at that, can it ever do something useful? It turns out <!--, with enough money 
          poured into development and interested academics, --> some neural network 
          frameworks are amazingly
          good at classifying certain types of data, and this is accomplished with 
          some rather straight-forward and mathematically clean training techniques.
        </p>
        <p id="trainp2">
          To train a neural network, we need a set of training data and a set of test data. 
        </p>
        <h2 id="costtit"> Cost Function </h2>
          <p id=costp1>
            The accuracy of a particular instance of a neural network over a given set of data
            can be metrized in the following way: 
            <div id="costquad" class="eq">
              $$ 
                \mathcal{C}_{quadratic}=\frac{1}{2}\sum_{\text{Test Data}}\left|
                 x_{pred}^i - x_{label}^i \right|^2  
              $$
            </div>
            This effectively defines a 'cost' that can be used to model the accuracy in 
            a well-behaved and differentiable way. 
          </p>
             
        <h2 id="bptit">
          Backpropogation
        </h2>
          <p>
            Backpropogation is a way to calculate how tweaking a certain parameter 
            affects one of the model's output for a given input. It's really just
            an application of the chain rule from standard calculus on an 
            admittedly large system.
          </p>
        <img src="../../assets/images/neural/bbdiagtemp.svg" alt="Backpropogation path highlighted and annotated" width="87%" 
          height="auto" id="bppath">

         <h2 id="convtit">
           Convolutional Neural Networks
         </h2>
           <p>
             A convolutional network actually generates what are called feature
             maps at each layer. These feature maps are the result of convoluting
             hierarchical elements with either a lower order feature map; or at the
             base layer, convolution of some basic elements with the input map.
             The specific form of these elements are now the parameters under
             flux in the training process. These elements are also known as 
             filters.
           </p>
           <p>
             Generally speaking, the convolutional layers of this form work best
             for feature extraction; whereas, a regular 'fully connected' network
             is better for the classification decision. Hence, many CNNs actually
             are a combination of several convolutional layers attached to a 
             regular neural network.
           </p>
    </section>
    <section id="resources">
        <h1 id="restitle">
          Resources 
        </h1>
        <p id="rescon">
          Michael A. Nielsen has a great book on neural networks for free that can be
          found <a href="http://neuralnetworksanddeeplearning.com/">here</a>. Another
          good, free, online resource is Goodfellow, Bengio, and Courville's book which
          can be found <a href="https://www.deeplearningbook.org/">here</a>. 
          Coursera has plenty of wonderful interactive courses on neural 
          networks and machine learning.          
        </p>

        <p>
          Perhaps the most beautiful introduction to neural networks in machine learning is
          this <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">series</a> by 3blue1brown.
        </p>
    </section>
  </body>
</html>
