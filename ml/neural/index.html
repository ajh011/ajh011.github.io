<!DOCTYPE html>
<html>
  <head>
    <title>Neural Networks</title>
    <link href="../../styles/neuralstyle.css" rel="stylesheet">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>MathJax.Hub.Config({  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}},
                                  loader: {load: ['[tex]/ams']},
                                  tex: {packages: {'[+]': ['ams']}});</script>
    
    <meta charset="UTF-8">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Monsieur+La+Doulaise&display=swap" rel="stylesheet">

    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="../../assets/images/iconwhite/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../../assets/images/iconwhite/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../../assets/images/iconwhite/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../assets/images/iconwhite/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="../../assets/images/iconwhite/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="../../assets/images/iconwhite/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="../../assets/images/iconwhite/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../../assets/images/iconwhite/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="../../assets/images/iconwhite/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="../../assets/images/iconwhite/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="../../assets/images/iconwhite/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="../../assets/images/iconwhite/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="../../assets/images/iconwhite/favicon-128.png" sizes="128x128" />
    <meta name="application-name" content="&nbsp;"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="../../assets/images/iconwhite/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="../../assets/images/iconwhite/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="../../assets/images/iconwhite/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo" content="../../assets/images/iconwhite/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo" content="../../assets/images/iconwhite/mstile-310x310.png" />



    <meta charset="UTF-8">
    <meta name="description" content="A discussion of why neural networks are interesting. Includes
                                      relevant resources for learning about neural networks
                                      and programming them in python.">
    <meta name="keywords" content="Neural Networks, Neural Nets, python Neural Nets,
                                   pytorch">
    <meta name="author" content="Alexander J. Heilman">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow">
  </head>
  
  <body>
    <h1 id="title">
      Neural Networks
    </h1>  
    <p id="explanation">
      Neural Networks have become a mainstay of modern machine learning. They're 
      especially interesting because of their basic and understandable components,
      as well as their loose association with an idealized brain. They're easily implementable
      on pytorch and tensorflow, both python based machine learning modules.  
    </p>
    <img class="center" src="../../assets/images/neural/ffnwhite.svg" alt="Example neural network" width="87%" height="auto" id="examplenn">
    <section id="basics">
      <h1 id="basicstit"> Neural Network Components  </h1>
      <h2 id="neuronstit">Neurons</h2>
      <p >
        <div id="nleft" class="">
        The neuron is the basic building block of a neural network. Generally,
        the mathematical functions it employs are relatively simple. The prototypical neuron
        simply multiplies all its inputs by a corresponding weight, sums up the products (it may add a bias as well),
        and then outputs this sum after applying some activation function.
        
        <img src="../../assets/images/neural/neuronwbsmatwhite.svg" alt="Example neuron with weights and biases in matrix form" 
           width="80%" height="auto" id="exneuron" class="center"> 
        </div>
      </p>
      <h3 id="activationfunctionstit">
        Activation Functions
      </h3>
      <p id="afp1">
        The activation function employed by a neuron in a neural net is generally simple: both to aid
        in computational complexity concerns when applying the model; and to streamline the 
        process of backpropogation, described later. Perhaps the two most commonly seen activation
        functions are the logarithmic Sigmoid function and the more modern and widely
        used rectified linear unit (ReLU).
      </p>
    <!--  <a href="https://commons.wikimedia.org/wiki/File:Sigmoid-function-2.svg" id="sigmoidlink" target="_blank" class="">    sigmoid     </a>
     -->
      <div id="actex">
        <img src="../../assets/images/neural/sigmoid.svg" alt="Example Sigmoid function plot" width="45%" height="auto" class="margin5">
        <img id="relu" src="../../assets/images/neural/relu.svg" alt="Example rectified linear unit function plot" width="45%" height="auto" class="margin5">
      </div>
    </section>
      <p id="nnbp1">
        <h2> Layers </h2>
        <img src="../../assets/images/neural/ffnwhiteinput.svg" alt="Basic computation done by a layer in a neural network" width="45%" height="auto"
          id="ffninput"> 
        <div id="nnbpright" >
          In between each intermediate layer of the neural network, the aggregation of
          inputs can be seen as a matrix acting
          on some input vector, as a natural extension of the single neuron case.
          The activation function (assuming it's the same for all the neurons) 
          can then simply act on this transformed vector via distribution.
        </div>
        <!--<iframe width="100%" height="400" src="https://www.desmos.com/calculator/qvltyv82p3" id="softmaxinteractive"></iframe>-->
      </p>
        <iframe src="https://www.desmos.com/calculator/wdipmg9s0q?embed" width="100%" height="400" invertedColors="true" style="border: 1px solid #ccc" id="softmaxinteractive" frameborder=0></iframe>
      <p id="bbnp2">
      <!-- words wrap left--> <img src="../../assets/images/neural/ffnwhiteoutput.svg" alt="" >
      </p>
    </section>
    <section id="training">
      <h1 id="traintit">Training</h1>
        <p id="trainp1"> 
          A neural network is analagous to a, potentially huge, set
          of knobs and dials. At this point, one may ask how we could ever 
          adjust such a large number of parameters to actually do something useful? And 
          at that, can it ever do something useful? It turns out <!--, with enough money 
          poured into development and interested academics, --> some neural network 
          frameworks are amazingly
          good at classifying certain types of data, and this is accomplished with 
          some rather straight-forward and mathematically clean training techniques.
        </p>
        <p id="trainp2">
          To train a neural network, we need a set of training data and a set of test data. 
        </p>
        <h2 id="costtit"> Cost Function </h2>
          <p id=costp1>
            The accuracy of a particular instance of a neural network over a given set of data
            can be metrized in the following way: 
            <div id="costquad" class="eq">
              $$ 
                \mathcal{C}_{quadratic}=\frac{1}{2}\sum_{\text{Test Data}}\left|
                 x_{pred}^i - x_{label}^i \right|^2  
              $$
            </div>
            This effectively defines a 'cost' that can be used to model the accuracy in 
            a well-behaved and differentiable way. 
          </p>
             
        <h2 id="bptit">
          Backpropogation
        </h2>
        <img src="../../assets/images/neural/bbdiagtemp.svg" alt="Backpropogation path highlighted and annotated" width="87%" 
          height="auto" id="bppath">
    </section>
    <section id="resources">
        <h1 id="restitle">
          Resources 
        </h1>
        <p id="rescon">
          Michael A. Nielsen has a great book on neural networks for free that can be
          found <a href="http://neuralnetworksanddeeplearning.com/">here</a>. Another
          good, free, online resource is Goodfellow, Bengio, and Courville's book which
          can be found <a href="https://www.deeplearningbook.org/">here</a>.
        </p>
    </section>
  </body>
</html>
